{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e489132-5c57-42a2-9d77-b2431a51111d",
   "metadata": {},
   "source": [
    "# PDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4c9178-6467-4dab-ba0c-de94347b3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries importing\n",
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as F\n",
    "import math\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, BooleanType, DateType, ArrayType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "727e90c9-79f4-4df0-814d-e3b0f899a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Add here your team number teamx\n",
    "team = 22\n",
    "\n",
    "# location of your Hive database in HDFS\n",
    "warehouse = \"/user/team22/project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\")\\\n",
    "        .config(\"spark.executor.cores\", 10)\\\n",
    "        .config(\"spark.executor.memory\", \"10g\")\\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dccb22d-e529-4d40-bd6c-f78d3d2355f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+-----------+\n",
      "|       namespace| tableName|isTemporary|\n",
      "+----------------+----------+-----------+\n",
      "|team22_projectdb|   flights|      false|\n",
      "|team22_projectdb|q1_results|      false|\n",
      "|team22_projectdb|q2_results|      false|\n",
      "|team22_projectdb|q3_results|      false|\n",
      "|team22_projectdb|q4_results|      false|\n",
      "|team22_projectdb|q5_results|      false|\n",
      "|team22_projectdb|q6_results|      false|\n",
      "+----------------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark.sql(\"SHOW DATABASES\").show()\n",
    "spark.sql(\"USE team22_projectdb\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402d6248-71e8-43f4-b888-0a8781ab141d",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41db6bae-ccf6-48ed-903f-82829c23b863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "df = spark.sql(\"SELECT * FROM team22_projectdb.flights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34018f06-336b-4476-b401-0ccb2bc6a0ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 6311871\n",
      "Columns: 61\n"
     ]
    }
   ],
   "source": [
    "# Shape\n",
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "print(f'Rows: {rows}\\nColumns: {cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d587fc97-448f-4ce9-b55a-6e665c8085e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- flightdate: date (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- dest: string (nullable = true)\n",
      " |-- cancelled: boolean (nullable = true)\n",
      " |-- diverted: boolean (nullable = true)\n",
      " |-- crsdeptime: timestamp (nullable = true)\n",
      " |-- deptime: timestamp (nullable = true)\n",
      " |-- depdelayminutes: integer (nullable = true)\n",
      " |-- depdelay: integer (nullable = true)\n",
      " |-- arrtime: timestamp (nullable = true)\n",
      " |-- arrdelayminutes: integer (nullable = true)\n",
      " |-- airtime: integer (nullable = true)\n",
      " |-- crselapsedtime: integer (nullable = true)\n",
      " |-- actualelapsedtime: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- dayofmonth: integer (nullable = true)\n",
      " |-- dayofweek: integer (nullable = true)\n",
      " |-- marketing_airline_network: string (nullable = true)\n",
      " |-- operated_or_branded_code_share_partners: string (nullable = true)\n",
      " |-- dot_id_marketing_airline: long (nullable = true)\n",
      " |-- iata_code_marketing_airline: string (nullable = true)\n",
      " |-- flight_number_marketing_airline: long (nullable = true)\n",
      " |-- operating_airline: string (nullable = true)\n",
      " |-- dot_id_operating_airline: long (nullable = true)\n",
      " |-- iata_code_operating_airline: string (nullable = true)\n",
      " |-- tail_number: string (nullable = false)\n",
      " |-- flight_number_operating_airline: long (nullable = true)\n",
      " |-- originairportid: long (nullable = true)\n",
      " |-- originairportseqid: long (nullable = true)\n",
      " |-- origincitymarketid: long (nullable = true)\n",
      " |-- origincityname: string (nullable = true)\n",
      " |-- originstate: string (nullable = true)\n",
      " |-- originstatefips: integer (nullable = true)\n",
      " |-- originstatename: string (nullable = true)\n",
      " |-- originwac: integer (nullable = true)\n",
      " |-- destairportid: long (nullable = true)\n",
      " |-- destairportseqid: long (nullable = true)\n",
      " |-- destcitymarketid: long (nullable = true)\n",
      " |-- destcityname: string (nullable = true)\n",
      " |-- deststate: string (nullable = true)\n",
      " |-- deststatefips: integer (nullable = true)\n",
      " |-- deststatename: string (nullable = true)\n",
      " |-- destwac: integer (nullable = true)\n",
      " |-- depdel15: boolean (nullable = false)\n",
      " |-- departuredelaygroups: integer (nullable = true)\n",
      " |-- deptimeblk: string (nullable = true)\n",
      " |-- taxiout: integer (nullable = true)\n",
      " |-- wheelsoff: timestamp (nullable = true)\n",
      " |-- wheelson: timestamp (nullable = true)\n",
      " |-- taxiin: integer (nullable = true)\n",
      " |-- crsarrtime: timestamp (nullable = true)\n",
      " |-- arrdelay: integer (nullable = true)\n",
      " |-- arrdel15: boolean (nullable = false)\n",
      " |-- arrivaldelaygroups: integer (nullable = true)\n",
      " |-- arrtimeblk: string (nullable = true)\n",
      " |-- distancegroup: integer (nullable = true)\n",
      " |-- divairportlandings: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5f81059-b899-480b-9823-1a1e082cd1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------+----+---------+--------+-------------------+-------------------+---------------+--------+-------------------+---------------+-------+--------------+-----------------+--------+----+-------+----------+---------+-------------------------+---------------------------------------+------------------------+---------------------------+-------------------------------+-----------------+------------------------+---------------------------+-----------+-------------------------------+---------------+------------------+------------------+--------------------+-----------+---------------+---------------+---------+-------------+----------------+----------------+------------+---------+-------------+-------------+-------+--------+--------------------+----------+-------+-------------------+-------------------+------+-------------------+--------+--------+------------------+----------+-------------+------------------+-----+\n",
      "|flightdate|             airline|origin|dest|cancelled|diverted|         crsdeptime|            deptime|depdelayminutes|depdelay|            arrtime|arrdelayminutes|airtime|crselapsedtime|actualelapsedtime|distance|year|quarter|dayofmonth|dayofweek|marketing_airline_network|operated_or_branded_code_share_partners|dot_id_marketing_airline|iata_code_marketing_airline|flight_number_marketing_airline|operating_airline|dot_id_operating_airline|iata_code_operating_airline|tail_number|flight_number_operating_airline|originairportid|originairportseqid|origincitymarketid|      origincityname|originstate|originstatefips|originstatename|originwac|destairportid|destairportseqid|destcitymarketid|destcityname|deststate|deststatefips|deststatename|destwac|depdel15|departuredelaygroups|deptimeblk|taxiout|          wheelsoff|           wheelson|taxiin|         crsarrtime|arrdelay|arrdel15|arrivaldelaygroups|arrtimeblk|distancegroup|divairportlandings|month|\n",
      "+----------+--------------------+------+----+---------+--------+-------------------+-------------------+---------------+--------+-------------------+---------------+-------+--------------+-----------------+--------+----+-------+----------+---------+-------------------------+---------------------------------------+------------------------+---------------------------+-------------------------------+-----------------+------------------------+---------------------------+-----------+-------------------------------+---------------+------------------+------------------+--------------------+-----------+---------------+---------------+---------+-------------+----------------+----------------+------------+---------+-------------+-------------+-------+--------+--------------------+----------+-------+-------------------+-------------------+------+-------------------+--------+--------+------------------+----------+-------------+------------------+-----+\n",
      "|2021-11-30|American Airlines...|   DFW| EWR|    false|   false|2021-11-30 15:05:00|2021-11-30 15:01:00|              0|      -4|2021-11-30 19:04:00|              0|    166|           203|              183|    1372|2021|      4|        30|        2|                       AA|                                     AA|                   19805|                         AA|                           1240|               AA|                   19805|                         AA|     N998NN|                           1240|          11298|           1129806|             30194|Dallas/Fort Worth...|         TX|             48|          Texas|       74|        11618|         1161802|           31703|  Newark, NJ|       NJ|           34|   New Jersey|     21|   false|                  -1| 1500-1559|     11|2021-11-30 15:12:00|2021-11-30 18:58:00|     6|2021-11-30 19:28:00|     -24|   false|                -2| 1900-1959|            6|                 0|   11|\n",
      "+----------+--------------------+------+----+---------+--------+-------------------+-------------------+---------------+--------+-------------------+---------------+-------+--------------+-----------------+--------+----+-------+----------+---------+-------------------------+---------------------------------------+------------------------+---------------------------+-------------------------------+-----------------+------------------------+---------------------------+-----------+-------------------------------+---------------+------------------+------------------+--------------------+-----------+---------------+---------------+---------+-------------+----------------+----------------+------------+---------+-------------+-------------+-------+--------+--------------------+----------+-------+-------------------+-------------------+------+-------------------+--------+--------+------------------+----------+-------------+------------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1 sample\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85513254",
   "metadata": {},
   "source": [
    "The original dataset contains 6,311,871 rows and 61 columns, some of which have null values. Since nulls are contained in almost all rows for which the label value is True, it was decided not to drop such rows, but to perform custom imputing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb31691-afe7-4c13-b660-6e904c57644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropped rows\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "def fillna_mean(df, include=set()): \n",
    "    means = df.agg(*(\n",
    "        F.mean(x).alias(x) for x in df.columns if x in include\n",
    "    ))\n",
    "    return df.fillna(means.first().asDict())\n",
    "\n",
    "def fillna_mode(df, column): \n",
    "    moda = df.groupby(column).count()\\\n",
    "                             .orderBy('count', ascending=False)\\\n",
    "                             .select(column)\\\n",
    "                             .collect()[1][column]\n",
    "    return df.na.fill(value = moda, subset = [column])\n",
    "\n",
    "# Small number of rows containing nulls\n",
    "df = df.dropna(subset = [\"crselapsedtime\", 'divairportlandings'])\n",
    "\n",
    "# Fill DepTime nulls\n",
    "df = df.withColumn(\"deptime\", coalesce(df.deptime,\n",
    "                                         df.crsdeptime))\n",
    "# Fill ArrTime nulls\n",
    "df = df.withColumn(\"arrtime\", coalesce(df.arrtime,\n",
    "                                         df.crsarrtime))\n",
    " # Fill ActualElapsedTime nulls\n",
    "df = df.withColumn(\"actualelapsedtime\", coalesce(df.actualelapsedtime, \n",
    "                                                   df.crselapsedtime))\n",
    "# Fill null values with mean\n",
    "df = fillna_mean(df, ['taxiout', 'wheelsoff', 'wheelson', 'taxiin'])\n",
    "# Fill null values with mode\n",
    "df = fillna_mode(df, 'tail_number')\n",
    "\n",
    "# Fill AirTime null values based on airtime formula\n",
    "df = df.withColumn('tmp', (df['actualelapsedtime'] - df['taxiout'] - df['taxiin']))\n",
    "df = df.withColumn(\"airtime\", coalesce(df.airtime, \n",
    "                                         df.tmp)).drop('tmp')\n",
    "\n",
    "# Fill with zeroes and False\n",
    "df = df.na.fill(value = 0, subset = [\"depdelayminutes\", 'depdelay', \n",
    "                                       \"arrdelayminutes\", 'arrdelay',\n",
    "                                       'arrivaldelaygroups', \n",
    "                                       'departuredelaygroups'])\n",
    "df = df.na.fill(value = False, subset = ['depdel15', 'arrdel15'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b316e19-4d8c-4e36-8701-3d061d1b5d73",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc47d82c-cc31-4ead-aeea-6ca4d73f22af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical features: airline, marketing_airline_network, operated_or_branded_code_share_partners, iata_code_marketing_airline, operating_airline, iata_code_operating_airline, originstate, originstatename, deststate, deststatename, deptimeblk, arrtimeblk, divairportlandings, dot_id_marketing_airline, dot_id_operating_airline\n",
      "Categorical features with many uniques: origin, dest, tail_number, origincityname, destcityname\n",
      "Numerical features: depdelayminutes, depdelay, arrdelayminutes, airtime, crselapsedtime, actualelapsedtime, distance, year, flight_number_marketing_airline, flight_number_operating_airline, originairportid, originairportseqid, origincitymarketid, originstatefips, originwac, destairportid, destairportseqid, destcitymarketid, deststatefips, destwac, departuredelaygroups, taxiout, taxiin, arrdelay, arrivaldelaygroups, distancegroup\n",
      "Time features: crsdeptime, deptime, crsarrtime, arrtime, wheelson, wheelsoff\n",
      "Boolean features: diverted, depdel15, arrdel15\n",
      "Target: cancelled\n"
     ]
    }
   ],
   "source": [
    "# Features' categories\n",
    "numerical = ['depdelayminutes', 'depdelay', 'arrdelayminutes', 'airtime',\n",
    "             'crselapsedtime', 'actualelapsedtime', 'distance', 'year', \n",
    "              'flight_number_marketing_airline', 'flight_number_operating_airline', \n",
    "             'originairportid', 'originairportseqid', 'origincitymarketid', \n",
    "             'originstatefips', 'originwac', 'destairportid', 'destairportseqid', \n",
    "             'destcitymarketid', 'deststatefips', 'destwac',  \n",
    "             'departuredelaygroups', 'taxiout', 'taxiin', 'arrdelay', \n",
    "              'arrivaldelaygroups', 'distancegroup']\n",
    "\n",
    "cyclical = ['quarter', 'month', 'dayofmonth', 'dayofweek']\n",
    "time_features = ['crsdeptime', 'deptime', 'crsarrtime', 'arrtime',  'wheelson', 'wheelsoff']\n",
    "date_features = ['flightdate']\n",
    "boolean_features = ['diverted', 'depdel15','arrdel15']\n",
    "\n",
    "categorical_as_cont = ['origin', 'dest', 'tail_number', 'origincityname', \n",
    "                       'destcityname']\n",
    "categorical_ohe = ['airline', 'marketing_airline_network', \n",
    "                   'operated_or_branded_code_share_partners', \n",
    "                   'iata_code_marketing_airline', 'operating_airline', \n",
    "                   'iata_code_operating_airline', 'originstate', \n",
    "                   'originstatename', 'deststate', 'deststatename', \n",
    "                   'deptimeblk', 'arrtimeblk', \n",
    "                   'divairportlandings', 'dot_id_marketing_airline','dot_id_operating_airline']\n",
    "\n",
    "label = 'cancelled'\n",
    "\n",
    "print(f\"Categorical features: {', '.join(categorical_ohe)}\\n\" + \n",
    "      f\"Categorical features with many uniques: {', '.join(categorical_as_cont)}\\n\" + \n",
    "      f\"Numerical features: {', '.join(numerical)}\\n\" + \n",
    "      f\"Time features: {', '.join(time_features)}\\n\" + \n",
    "      f\"Boolean features: {', '.join(boolean_features)}\\n\" + \n",
    "      f\"Target: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d37507",
   "metadata": {},
   "source": [
    "After the stage of nulls elimination, one need to do proper sampling for dataset, as it is very imbalanced in term of label columns 'cancelled'. The reason for that is simply that dataset contains less records about cancelled flights, than successfully completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5377c5ca-3e0c-4ef2-9136-2af060c9164c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data as it's really imbalansed wrt label\n",
    "major_df = df.filter(F.col(label) == 'false')\n",
    "minor_df = df.filter(F.col(label) == 'true')\n",
    "\n",
    "result_frac = minor_df.count()/major_df.count()\n",
    "\n",
    "df_sampled = df.sampleBy(label, fractions={False: result_frac, True: 1}, seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35727c74",
   "metadata": {},
   "source": [
    "A common method for encoding different cyclical data, including date and time, is to transform this data into two dimensions using a sine and cosine transformations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2874726e-7031-4b52-b3cb-3ea3c6ec4c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to encode cyclical features with sin and cos\n",
    "class TimeTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_col = Param(Params._dummy(), \"input_col\", \"input column name.\", typeConverter=TypeConverters.toString)\n",
    "    output_col = Param(Params._dummy(), \"output_col\", \"output column name.\", typeConverter=TypeConverters.toString)\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        super(TimeTransformer, self).__init__()\n",
    "        self._setDefault(input_col=None, output_col=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "        self.set_coef()\n",
    "\n",
    "    def set_coef(self, coef: float = 12.0):\n",
    "        self.coef = coef\n",
    "        return self\n",
    "\n",
    "    @keyword_only\n",
    "    def set_params(self, input_col: str = \"input\", output_col: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "\n",
    "    def get_input_col(self):\n",
    "        return self.getOrDefault(self.input_col)\n",
    "\n",
    "    def get_output_col(self):\n",
    "        return self.getOrDefault(self.output_col)\n",
    "\n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_col = self.get_input_col()\n",
    "        output_col = self.get_output_col()\n",
    "        output_col_cos = output_col + \"_cos\"\n",
    "        output_col_sin = output_col + \"_sin\"\n",
    "\n",
    "        transform_udf = F.udf(lambda x: str(2*math.pi*int(x)/self.coef), StringType())\n",
    "        return df.withColumn(output_col_cos, F.cos(transform_udf(input_col)))\\\n",
    "                 .withColumn(output_col_sin, F.sin(transform_udf(input_col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5722acd4",
   "metadata": {},
   "source": [
    "The main pipeline for features extraction includes indexers, time transformer, one-hot encoder and minmax scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21b1c144-258d-4e15-9aa3-4441f2402992",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Convert boolean\n",
    "for b in boolean_features + [label]:\n",
    "    df_sampled = df_sampled.withColumn(b, F.col(b).cast('integer'))\n",
    "\n",
    "# String indexers for categorical features\n",
    "indexer = StringIndexer()\n",
    "indexer.setHandleInvalid(\"keep\")\n",
    "indexer.setInputCols(categorical_ohe)\n",
    "features_idx = list(map(lambda x : x + \"_idx\", categorical_ohe))\n",
    "indexer.setOutputCols(features_idx)\n",
    "\n",
    "cat_indexer = StringIndexer()\n",
    "cat_indexer.setHandleInvalid(\"keep\")\n",
    "cat_indexer.setInputCols(categorical_as_cont)\n",
    "features_cat_idx = list(map(lambda x : x + \"_idx\", categorical_as_cont))\n",
    "cat_indexer.setOutputCols(features_cat_idx)\n",
    "\n",
    "# One-hot encoding for categorical\n",
    "encoders = []\n",
    "for f in features_idx:\n",
    "    encoders.append(OneHotEncoder(inputCol = f, outputCol = f + \"_enc\"))\n",
    "\n",
    "# Encode parsed cyclical\n",
    "period = {\n",
    "    \"quarter\": 4, \n",
    "    \"month\": 12, \n",
    "    \"dayofmonth\": 31, \n",
    "    \"dayofweek\": 7\n",
    "}\n",
    "cyclical_encoders = []\n",
    "for c in cyclical:\n",
    "    cyclical_encoders.append(TimeTransformer(input_col = c, output_col = c).set_coef(coef = period[c]))\n",
    "    \n",
    "# Encode time\n",
    "time_idx = []\n",
    "time_encoders = []\n",
    "for t in time_features:\n",
    "    time_idx.append(t + '_hour')\n",
    "    time_idx.append(t + '_minute')\n",
    "    time_idx.append(t + '_day')\n",
    "    time_idx.append(t + '_month')\n",
    "\n",
    "    df_sampled = df_sampled.withColumn(t + '_hour', F.hour(t))\n",
    "    df_sampled = df_sampled.withColumn(t + '_minute', F.minute(t))\n",
    "    df_sampled = df_sampled.withColumn(t + '_day', F.dayofmonth(t))\n",
    "    df_sampled = df_sampled.withColumn(t + '_month', F.month(t))\n",
    "    \n",
    "    time_encoders.append(TimeTransformer(input_col = t + '_hour', output_col = t + '_hour').set_coef(coef = 24.0))\n",
    "    time_encoders.append(TimeTransformer(input_col = t + '_minute', output_col = t + '_minute').set_coef(coef = 60.0))\n",
    "    time_encoders.append(TimeTransformer(input_col = t + '_day', output_col = t + '_day').set_coef(coef = 31.0))\n",
    "    time_encoders.append(TimeTransformer(input_col = t + '_month', output_col = t + '_month').set_coef(coef = 12.0))\n",
    "\n",
    "# Encode date\n",
    "date_idx = []\n",
    "date_encoders = []\n",
    "for d in date_features:\n",
    "    date_idx.append(d + '_day')\n",
    "    date_idx.append(d + '_month')\n",
    "    \n",
    "    df_sampled = df_sampled.withColumn(d + '_day', F.dayofmonth(t))\n",
    "    df_sampled = df_sampled.withColumn(d + '_month', F.month(t))\n",
    "    \n",
    "    date_encoders.append(TimeTransformer(input_col = d + '_day', \n",
    "                                         output_col = d + '_day').set_coef(coef = 31.0))\n",
    "    date_encoders.append(TimeTransformer(input_col = d + '_month', \n",
    "                                         output_col = d + '_month').set_coef(coef = 12.0))\n",
    "\n",
    "\n",
    "# Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols = [f + \"_enc\" for f in features_idx] + features_cat_idx + numerical + boolean_features + [f + \"_sin\" for f in time_idx] + \\\n",
    "                [f + \"_cos\" for f in time_idx] + [f + \"_sin\" for f in date_idx] + [f + \"_cos\" for f in date_idx] + [f + \"_sin\" for f in cyclical] + \\\n",
    "                [f + \"_cos\" for f in cyclical],\n",
    "    outputCol='features_unscaled'\n",
    "    )\n",
    "\n",
    "# MinMax Scaler\n",
    "scaler = MinMaxScaler(inputCol = \"features_unscaled\", outputCol = \"features\")\n",
    "\n",
    "# Apply pipeline\n",
    "pipeline = Pipeline(stages = [indexer, cat_indexer] + encoders + time_encoders + date_encoders + cyclical_encoders + [assembler, scaler])\n",
    "features_pipeline_model = pipeline.fit(df_sampled)\n",
    "df_enc = features_pipeline_model.transform(df_sampled)\n",
    "\n",
    "# Indexing the labels\n",
    "label_indexer = StringIndexer()\n",
    "label_indexer.setInputCol(label)\n",
    "label_indexer.setOutputCol('label')\n",
    "label_idx_model = label_indexer.fit(df_enc)\n",
    "\n",
    "# Apply the indexer\n",
    "df_labeled = label_idx_model.transform(df_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ec5adcc-1bce-468c-8bc2-ad4027ce0809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  0.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "|(481,[11,22,33,47...|  0.0|\n",
      "|(481,[11,22,33,47...|  1.0|\n",
      "+--------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get features and label\n",
    "df_proj = df_labeled.select('features', 'label')\n",
    "df_proj.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1ac48a7-66a1-43ca-ba8b-61167191d3cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of new features\n",
    "df_proj.select('features').collect()[0]['features'].toArray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5820063a-a630-41ea-886d-dd283a1031c4",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cfd121a-8e2d-46bd-b4ae-71925b18721f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ratio: 0.7\n",
      "Train size: 155402\n",
      "Test size: 66485\n"
     ]
    }
   ],
   "source": [
    "# Train/Test split\n",
    "trainRatio = 0.7\n",
    "\n",
    "train_df, test_df = df_proj.randomSplit([trainRatio, 1 - trainRatio], seed = 42)\n",
    "print(f\"Ratio: {trainRatio}\\nTrain size: {train_df.count()}\\nTest size: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93d2e401-e31c-4f18-b59a-d6bcf41f37a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A function to run commands\n",
    "import os\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "# Save to HDFS\n",
    "train_df.select(\"features\", \"label\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format('json')\\\n",
    "    .save(\"/user/team22/project/data/train\")\n",
    "\n",
    "# Add localy\n",
    "run(\"hdfs dfs -cat /user/team22/project/data/train/*.json > ~/bigdata-final-project/data/train.json\")\n",
    "\n",
    "# Save to HDFS\n",
    "test_df.select(\"features\", \"label\")\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"/user/team22/project/data/test\")\n",
    "\n",
    "# Add localy\n",
    "run(\"hdfs dfs -cat /user/team22/project/data/test/*.json > ~/bigdata-final-project/data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd615e06-a7fe-4cf8-a5ee-358a237ac082",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42711d",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble learning method that combines multiple decision trees to create a more robust and accurate model. It builds multiple decision trees to improve the overall performance using a subset of the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce94d061-8770-4f01-8ae7-df2fa38e6844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train first model\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_calssifier = RandomForestClassifier()\n",
    "rf_model = rf_calssifier.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bdf41fa9-e643-4bed-a678-c19552a698f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(481,[0,24,32,49,...|  1.0|[1.69139069648060...|[0.08456953482403...|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.3256412923311...|[0.96628206461655...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.3256266565640...|[0.96628133282820...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.5285136831634...|[0.97642568415817...|       0.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[0.81929420337911...|[0.04096471016895...|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.6574881924735...|[0.98287440962367...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.6578216672810...|[0.98289108336405...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.8502231911957...|[0.99251115955978...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.3374913360979...|[0.96687456680489...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[19.3374913360979...|[0.96687456680489...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test first model\n",
    "rf_predictions = rf_model.transform(test_df)\n",
    "rf_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0337ef8d-4e02-405e-989e-5ca808deefec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test area under ROC: 0.9990340030748656\n",
      "Test area under PR: 0.9987893832699755\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "rf_evaluator_roc = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "rf_roc = rf_evaluator_roc.evaluate(rf_predictions)\n",
    "\n",
    "rf_evaluator_pr = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderPR\")\n",
    "\n",
    "rf_pr = rf_evaluator_pr.evaluate(rf_predictions)\n",
    "\n",
    "print(f\"Test area under ROC: {rf_roc}\\nTest area under PR: {rf_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc8879b1-6a51-490b-8e13-a03f7cb33185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "\t maxDepth - [5, 10]\n",
      "\t impurity - ['entropy', 'gini']\n",
      "\t numTrees - [10, 20]\n",
      "\n",
      "Best hyperparameters:\n",
      "\t maxDepth - 10\n",
      "\t impurity - gini\n",
      "\t numTrees - 10\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassificationModel: uid=RandomForestClassifier_a343737ec309, numTrees=10, numClasses=2, numFeatures=481"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameter optimization\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "print(f\"Hyperparameters:\\n\\t maxDepth - {[5, 10]}\\n\\t impurity - {['entropy', 'gini']}\\n\\t numTrees - {[10, 20]}\")\n",
    "\n",
    "rf_grid = ParamGridBuilder()\n",
    "rf_grid = rf_grid.addGrid(rf_model.maxDepth, [5, 10])\\\n",
    "                 .addGrid(rf_model.impurity, ['entropy', 'gini'])\\\n",
    "                 .addGrid(rf_model.numTrees, [10, 20])\\\n",
    "                 .build()\n",
    "\n",
    "rf_cv = CrossValidator(estimator = rf_calssifier,\n",
    "                    estimatorParamMaps = rf_grid,\n",
    "                    evaluator = rf_evaluator_roc,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "rf_cvModel = rf_cv.fit(train_df)\n",
    "model1 = rf_cvModel.bestModel\n",
    "\n",
    "print(f\"\\nBest hyperparameters:\\n\\t maxDepth - {model1.getMaxDepth()}\\n\\t impurity - {model1.getImpurity()}\\n\\t numTrees - {model1.getNumTrees}\\n\")\n",
    "\n",
    "model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f41e209c-7d50-48ae-af27-d60a1b0c6e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.write().overwrite().save(\"/user/team22/project/models/model1\")\n",
    "run(\"hdfs dfs -get /user/team22/project/models/model1 ~/bigdata-final-project/models/model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "504b1ea8-3869-4d97-8990-5f02090fc1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(481,[0,24,32,49,...|  1.0|          [0.0,10.0]|           [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  1.0|          [0.0,10.0]|           [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[9.99977270144334...|[0.99997727014433...|       0.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_predictions_grid = model1.transform(test_df)\n",
    "rf_predictions_grid.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fab41130-2921-424e-8503-366e16191c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_predictions_grid.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/team22/project/output/model1_predictions\")\n",
    "run(\"hdfs dfs -cat /user/team22/project/output/model1_predictions/*.csv > ~/bigdata-final-project/output/model1_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "391f9380-5531-4ed9-96b9-9cb24269e307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test area under ROC for best model: 0.9999400910615863\n",
      "Test area under PR for best model: 0.9999701718559475\n"
     ]
    }
   ],
   "source": [
    "rf_evaluator_best_roc = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "rf_best_roc = rf_evaluator_best_roc.evaluate(rf_predictions_grid)\n",
    "\n",
    "rf_evaluator_best_pr = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderPR\")\n",
    "\n",
    "rf_best_pr = rf_evaluator_best_pr.evaluate(rf_predictions_grid)\n",
    "\n",
    "print(f\"Test area under ROC for best model: {rf_best_roc}\\nTest area under PR for best model: {rf_best_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bce50e0-263a-4474-b962-581e87f38d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test area under ROC for first model: 0.9990340030748656\n",
      "Test area under PR for first model: 0.9987893832699755\n",
      "Test area under ROC for best model: 0.9999400910615863\n",
      "Test area under PR for best model: 0.9999701718559475\n",
      "\n",
      "Test area under ROC increased by 0.0009060879867207605 with optimization\n",
      "Test area under PR increased by 0.0011807885859720368 with optimization\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test area under ROC for first model: {rf_roc}\\nTest area under PR for first model: {rf_pr}\")\n",
    "print(f\"Test area under ROC for best model: {rf_best_roc}\\nTest area under PR for best model: {rf_best_pr}\")\n",
    "print(f\"\\nTest area under ROC increased by {rf_best_roc - rf_roc} with optimization\\nTest area under PR increased \" + \n",
    "      f\"by {rf_best_pr - rf_pr} with optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4760ea4-f9ed-4cac-8048-91c6c85548b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c6df83",
   "metadata": {},
   "source": [
    "The Factorization Machines algorithm is a supervised learning algorithm which is an extension of a linear model designed to capture interactions between features within high dimensional sparse datasets economically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ed188b0-037b-49d7-9c74-26c283c58ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train second model\n",
    "from pyspark.ml.classification import FMClassifier\n",
    "\n",
    "fm_calssifier = FMClassifier()\n",
    "fm_model = fm_calssifier.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "67472c32-30d7-46aa-bb01-1f5cf23c2f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+-----------+----------+\n",
      "|            features|label|       rawPrediction|probability|prediction|\n",
      "+--------------------+-----+--------------------+-----------+----------+\n",
      "|(481,[0,24,32,49,...|  1.0|[-544.39373904847...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-2778.9815954978...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-1510.2971542198...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-1666.4601865016...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-485.29834042243...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-3013.5754113979...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-4548.9441311390...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-3781.1434407947...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-1910.1580735405...|  [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-3206.8077174193...|  [0.0,1.0]|       1.0|\n",
      "+--------------------+-----+--------------------+-----------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test second model\n",
    "fm_predictions = fm_model.transform(test_df)\n",
    "fm_predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5770b8d3-160c-4a5c-93ed-3244d9683ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test area under ROC: 0.9988274728734019\n",
      "Test area under PR: 0.9988706038360329\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "fm_evaluator_roc = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "fm_roc = fm_evaluator_roc.evaluate(fm_predictions)\n",
    "\n",
    "fm_evaluator_pr = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderPR\")\n",
    "\n",
    "fm_pr = fm_evaluator_pr.evaluate(fm_predictions)\n",
    "\n",
    "print(f\"Test area under ROC: {fm_roc}\\nTest area under PR: {fm_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "05a29786-6210-4080-9a3c-ca4ebbf6f72f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "\t regParam - [0.0, 0.5]\n",
      "\t initStd - [0.01, 0.05]\n",
      "\t factorSize - [4, 8]\n",
      "\n",
      "Best hyperparameters:\n",
      "\t initStd - 0.05\n",
      "\t regParam - 0.0\n",
      "\t factorSize - 8\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FMClassificationModel: uid=FMClassifier_3c5286ea03fb, numClasses=2, numFeatures=481, factorSize=8, fitLinear=true, fitIntercept=true"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyper-parameter optimization\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "print(f\"Hyperparameters:\\n\\t regParam - {[0.0, 0.5]}\\n\\t initStd - {[0.01, 0.05]}\\n\\t factorSize - {[4, 8]}\")\n",
    "\n",
    "fm_grid = ParamGridBuilder()\n",
    "fm_grid = fm_grid.addGrid(fm_model.regParam, [0.0, 0.5])\\\n",
    "                 .addGrid(fm_model.initStd, [0.01, 0.05])\\\n",
    "                 .addGrid(fm_model.factorSize, [4, 8])\\\n",
    "                 .build()\n",
    "\n",
    "fm_cv = CrossValidator(estimator = fm_calssifier,\n",
    "                    estimatorParamMaps = fm_grid,\n",
    "                    evaluator = fm_evaluator_roc,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "fm_cvModel = fm_cv.fit(train_df)\n",
    "model2 = fm_cvModel.bestModel\n",
    "\n",
    "print(f\"\\nBest hyperparameters:\\n\\t initStd - {model2.getInitStd()}\\n\\t regParam - {model2.getRegParam()}\\n\\t factorSize - {model2.getFactorSize()}\\n\")\n",
    "\n",
    "model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e51d1bba-9396-42c7-b432-934128f4e85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.write().overwrite().save(\"/user/team22/project/models/model2\")\n",
    "run(\"hdfs dfs -get /user/team22/project/models/model2 ~/bigdata-final-project/models/model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b67a751-9258-48f6-8524-a8469d316b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|            features|label|       rawPrediction|         probability|prediction|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "|(481,[0,24,32,49,...|  0.0|[597.955625362807...|[1.0,2.0472528384...|       0.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-674.62556143144...|           [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-1117.4210474668...|           [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[836.823753741428...|           [1.0,0.0]|       0.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[805.559720376552...|           [1.0,0.0]|       0.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-709.01286573540...|           [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[309.718960200438...|[1.0,3.0957411791...|       0.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-1600.6903232091...|           [0.0,1.0]|       1.0|\n",
      "|(481,[0,24,32,49,...|  0.0|[376.616634874766...|[1.0,2.7382536857...|       0.0|\n",
      "|(481,[0,24,32,49,...|  1.0|[-1111.6856974212...|           [0.0,1.0]|       1.0|\n",
      "+--------------------+-----+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fm_predictions_grid = model2.transform(test_df)\n",
    "fm_predictions_grid.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "164b4c88-3090-47dd-acf4-3e624e6d171d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fm_predictions_grid.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/team22/project/output/model2_predictions\")\n",
    "run(\"hdfs dfs -cat /user/team22/project/output/model2_predictions/*.csv > ~/bigdata-final-project/output/model2_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1070b918-22d7-465c-906a-2b80ec98b7ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test area under ROC for best model: 0.999143825636828\n",
      "Test area under PR for best model: 0.9989817614080468\n"
     ]
    }
   ],
   "source": [
    "fm_evaluator_best_roc = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderROC\")\n",
    "\n",
    "fm_best_roc = fm_evaluator_best_roc.evaluate(fm_predictions_grid)\n",
    "\n",
    "fm_evaluator_best_pr = BinaryClassificationEvaluator()\\\n",
    "  .setLabelCol(\"label\")\\\n",
    "  .setRawPredictionCol(\"prediction\")\\\n",
    "  .setMetricName(\"areaUnderPR\")\n",
    "\n",
    "fm_best_pr = fm_evaluator_best_pr.evaluate(fm_predictions_grid)\n",
    "\n",
    "print(f\"Test area under ROC for best model: {fm_best_roc}\\nTest area under PR for best model: {fm_best_pr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0795c4e3-b1ba-4ca7-9fb5-2f7ec651e5e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test area under ROC for first model: 0.9988274728734019\n",
      "Test area under PR for first model: 0.9988706038360329\n",
      "Test area under ROC for best model: 0.999143825636828\n",
      "Test area under PR for best model: 0.9989817614080468\n",
      "\n",
      "Test area under ROC increased by 0.00031635276342611984 with optimization\n",
      "Test area under PR increased by 0.00011115757201396459 with optimization\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test area under ROC for first model: {fm_roc}\\nTest area under PR for first model: {fm_pr}\")\n",
    "print(f\"Test area under ROC for best model: {fm_best_roc}\\nTest area under PR for best model: {fm_best_pr}\")\n",
    "print(f\"\\nTest area under ROC increased by {fm_best_roc - fm_roc} with optimization\\nTest area under PR increased \" + \n",
    "      f\"by {fm_best_pr - fm_pr} with optimization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d25145c-cd96-425d-8bb6-de1315edbcfe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8817f132-089a-4d1f-b480-717f5dbd9db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|model                                                                                                                               |Area under ROC    |Area under PR     |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "|RandomForestClassificationModel: uid=RandomForestClassifier_a343737ec309, numTrees=10, numClasses=2, numFeatures=481                |0.9999400910615863|0.9999701718559475|\n",
      "|FMClassificationModel: uid=FMClassifier_3c5286ea03fb, numClasses=2, numFeatures=481, factorSize=8, fitLinear=true, fitIntercept=true|0.999143825636828 |0.9989817614080468|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data frame to report performance of the models\n",
    "models = [\n",
    "    [str(model1),rf_best_roc, rf_best_pr], \n",
    "    [str(model2),fm_best_roc, fm_best_pr]\n",
    "]\n",
    "\n",
    "result_df = spark.createDataFrame(models, [\"model\", \"Area under ROC\", \"Area under PR\"])\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5f69a95-aa36-4b51-84bc-2e9f83fdb51f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save it to HDFS\n",
    "result_df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/team22/project/output/evaluation\")\n",
    "run(\"hdfs dfs -cat /user/team22/project/output/evaluation/*.csv > ~/bigdata-final-project/output/evaluation.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b86811-1c7b-4bf4-a59b-935b01707ce9",
   "metadata": {},
   "source": [
    "## Store additional results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dd53f8",
   "metadata": {},
   "source": [
    "### Get best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "41be203c-fb09-4cc8-9138-48b8820d9283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------+--------------+---------------+--------------+\n",
      "|model                                                                                                                               |Parameter 1   |Parameter 2    |Parameter 3   |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+--------------+---------------+--------------+\n",
      "|RandomForestClassificationModel: uid=RandomForestClassifier_a343737ec309, numTrees=10, numClasses=2, numFeatures=481                |maxDepth = 10 |impurity = gini|numTrees = 10 |\n",
      "|FMClassificationModel: uid=FMClassifier_3c5286ea03fb, numClasses=2, numFeatures=481, factorSize=8, fitLinear=true, fitIntercept=true|initStd = 0.05|regParam = 0.0 |factorSize = 8|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+--------------+---------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data frame to report performance of the models\n",
    "hypoparams = [\n",
    "    [str(model1),f'maxDepth = {model1.getMaxDepth()}', f'impurity = {model1.getImpurity()}', f'numTrees = {model1.getNumTrees}'], \n",
    "    [str(model2),f'initStd = {model2.getInitStd()}', f'regParam = {model2.getRegParam()}', f'factorSize = {model2.getFactorSize()}']\n",
    "]\n",
    "\n",
    "hypo_df = spark.createDataFrame(hypoparams, [\"model\", \"Parameter 1\", \"Parameter 2\", \"Parameter 3\"])\n",
    "hypo_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2b4346c3-1344-4984-be63-61fa52f13be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save it to HDFS\n",
    "hypo_df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/team22/project/output/hyperparameters\")\n",
    "run(\"hdfs dfs -cat /user/team22/project/output/hyperparameters/*.csv > ~/bigdata-final-project/output/hyperparameters.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2fb198",
   "metadata": {},
   "source": [
    "### Get result of optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "586e32bc-c505-4678-895d-fa66bab19f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------+----------------------+------------------------+--------------------------+---------------------+-----------------------+-------------------------+\n",
      "|model                                                                                                                               |Initial area under ROC|Optimized area under ROC|Increase of area under ROC|Initial area under PR|Optimized area under PR|Increase of area under PR|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+----------------------+------------------------+--------------------------+---------------------+-----------------------+-------------------------+\n",
      "|RandomForestClassificationModel: uid=RandomForestClassifier_a343737ec309, numTrees=10, numClasses=2, numFeatures=481                |0.9990340030748656    |0.9999400910615863      |9.060879867207605E-4      |0.9987893832699755   |0.9999701718559475     |0.0011807885859720368    |\n",
      "|FMClassificationModel: uid=FMClassifier_3c5286ea03fb, numClasses=2, numFeatures=481, factorSize=8, fitLinear=true, fitIntercept=true|0.9988274728734019    |0.999143825636828       |3.1635276342611984E-4     |0.9988706038360329   |0.9989817614080468     |1.1115757201396459E-4    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------+----------------------+------------------------+--------------------------+---------------------+-----------------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create data frame to report performance of the models\n",
    "optim_models = [\n",
    "    [str(model1),rf_roc, rf_best_roc, rf_best_roc - rf_roc, rf_pr, rf_best_pr, rf_best_pr - rf_pr], \n",
    "    [str(model2),fm_roc, fm_best_roc, fm_best_roc - fm_roc, fm_pr, fm_best_pr, fm_best_pr - fm_pr]\n",
    "]\n",
    "\n",
    "optim_df = spark.createDataFrame(optim_models, [\"model\", \"Initial area under ROC\", \"Optimized area under ROC\", \"Increase of area under ROC\", \n",
    "                                                \"Initial area under PR\", \"Optimized area under PR\", \"Increase of area under PR\"])\n",
    "optim_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64a25c44-e890-45e0-8acd-a681f321af44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save it to HDFS\n",
    "optim_df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/team22/project/output/optimization\")\n",
    "\n",
    "run(\"hdfs dfs -cat /user/team22/project/output/optimization/*.csv > ~/bigdata-final-project/output/optimization.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d180811-2c11-431e-b6c3-db9fd1c6257f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Predict sample on both models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "2f0d661e-1e88-42a5-8ef3-9e27a568da7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get random sample and predict on both models\n",
    "prediction_sample = df_labeled.sample(fraction=10/df_labeled.count(), seed=123)\n",
    "models_pred = model2.transform(model1.transform(prediction_sample)\\\n",
    "                 .withColumnRenamed(\"prediction\", \"model1_prediction\")\\\n",
    "                 .withColumnRenamed(\"rawPrediction\", \"model1_rawPrediction\")\\\n",
    "                 .withColumnRenamed(\"probability\", \"model1_probability\"))\\\n",
    "                 .withColumnRenamed(\"prediction\", \"model2_prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "fae9aaea-5c1c-4851-b3e9-bf85a2848d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_result = models_pred.select(*(numerical + cyclical +\\\n",
    "                                               time_features + date_features +\\\n",
    "                                               boolean_features + categorical_as_cont +\\\n",
    "                                               categorical_ohe + ['label', \"model1_prediction\", \"model2_prediction\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33fdb394-d4f8-4a14-a3ee-5455f0d02fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save it to HDFS\n",
    "prediction_result.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \";\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"/user/team22/project/output/prediction_samples\")\n",
    "\n",
    "run(\"hdfs dfs -cat /user/team22/project/output/prediction_samples/*.csv > ~/bigdata-final-project/output/prediction_samples.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
